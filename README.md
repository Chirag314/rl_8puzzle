# ğŸ§© RL 8-Puzzle â€” Q-Learning Agent + 3D Animated Solver

## ğŸ¥ Demo (click to play)

<p align="center">
  <video src="rl_8puzzle/media/solution_3d.mp4"
         controls loop muted playsinline width="480">
    Your browser does not support the video tag.
  </video>
</p>

<p align="center">
  âœ If the video player doesnâ€™t show up in your viewer, you can
  <a href="rl_8puzzle/media/solution_3d.mp4">click here to open / download the MP4</a>.
</p>

A fully working **Reinforcement Learning project** that trains an agent to solve the classic **8-Puzzle** (3Ã—3 sliding tile) using **tabular Q-Learning**, and then visualizes the solution in a smooth **3D animation**.

Each run can:

- Generate a **new solvable puzzle**
- Compute a **solution trajectory** using the learned policy
- Produce a **3D MP4 animation** of the blocks sliding into place

---

## ğŸ”– Badges

<p align="center">
  <img src="https://img.shields.io/badge/python-3.10%2B-blue" />
  <img src="https://img.shields.io/badge/reinforcement%20learning-Q--learning-orange" />
  <img src="https://img.shields.io/badge/tests-passing-brightgreen" />
  <img src="https://img.shields.io/badge/license-MIT-purple" />
</p>

---

## ğŸŒŸ Features

- âœ” Q-Learning agent on the 8-puzzle MDP  
- âœ” Automatically generates **new puzzles on each run**  
- âœ” Full **3D sliding-block animation** (MP4, optional GIF)  
- âœ” Command-line interface (CLI) with configurable difficulty  
- âœ” Tests with `pytest`  
- âœ” Example solution video embedded above  

---

## ğŸ“¦ Project Structure

    rl_8puzzle/
    â”‚
    â”œâ”€ __main__.py               # Entry point (supports CLI flags)
    â”œâ”€ env.py                    # 3Ã—3 8-Puzzle environment
    â”œâ”€ n_puzzle_env.py           # Optional NÃ—N environment
    â”œâ”€ train_q_learning.py       # Tabular Q-learning logic
    â”œâ”€ animate_3d.py             # 3D animation engine (MP4 + GIF)
    â”œâ”€ runner.py                 # "new puzzle + animation" pipeline
    â”‚
    â”œâ”€ q_table.pkl               # (generated) learned Q-values
    â”œâ”€ used_start_states.json    # (generated) avoids duplicate puzzles
    â”‚
    â”œâ”€ media/
    â”‚   â””â”€ solution_3d.mp4
    â”‚
    â””â”€ tests/
        â”œâ”€ test_env_8puzzle.py
        â””â”€ test_q_learning_8puzzle.py

---

## ğŸ§  Reinforcement Learning Formulation

### State representation

A state is a 9-tuple:

    (1, 2, 3,
     4, 5, 6,
     7, 8, 0)

where `0` is the blank tile.

The goal state is:

    (1, 2, 3,
     4, 5, 6,
     7, 8, 0)

---

### Action space

The agent acts on the blank tile:

| Action | Meaning        |
|--------|----------------|
| 0      | Move blank up  |
| 1      | Move blank down|
| 2      | Move blank left|
| 3      | Move blank right|

If an action is illegal (e.g. moving up when the blank is already in the top
row), the environment simply leaves the state unchanged.

---

### Reward function

- Every step: `âˆ’1`  
- Reaching the goal state: `+20`  

This encourages **shortest-path** solutions and penalizes wandering.

---

### Episodes

- Start from a scrambled state generated by applying a fixed number of random
  legal moves from the goal.
- End when:
  - the goal is reached, or
  - a maximum step limit is exceeded.

---

### Q-Learning

We use tabular Q-learning with the standard update:

    Q(s, a) â† Q(s, a) + Î± [ r + Î³ max_a' Q(s', a') âˆ’ Q(s, a) ]

Policy:

- During training: Îµ-greedy exploration  
- During evaluation / animation: purely greedy (`argmax_a Q(s, a)`)

The learned Q-table is stored in:

    q_table.pkl

---

## ğŸš€ Quick Start

From the repository root, run:

    python -m rl_8puzzle

This will:

1. Load an existing Q-table from `rl_8puzzle/q_table.pkl`, or train one if it
   does not exist.
2. Generate a scrambled but solvable puzzle.
3. Solve it using the greedy policy derived from the Q-table.
4. Render a 3D animation of the solution.

### Output

The video is written to:

    rl_8puzzle/solution_3d.mp4

Running the command again will attempt to generate a **different puzzle** (see
â€œNew puzzle every runâ€ below).

---

## ğŸ› Command-Line Arguments (CLI)

You can customize the puzzle difficulty, animation smoothness, and output
location.

Example:

    python -m rl_8puzzle \
        --scramble 60 \
        --min-moves 15 \
        --fps 10 \
        --substeps 12 \
        --output rl_8puzzle/media/my_video.mp4

Available flags:

| Flag             | Description                                         |
|------------------|-----------------------------------------------------|
| `--scramble N`   | Number of random moves used to scramble the puzzle |
| `--min-moves N`  | Reject puzzles with solution shorter than N moves   |
| `--fps N`        | Frames per second in the output video               |
| `--substeps N`   | Interpolated frames between discrete moves          |
| `--output PATH`  | Path of the output MP4 file                         |
| `--gif`          | Also render a GIF alongside the MP4                 |

---

## ğŸ¥ GIF Generation

If GIF generation is enabled, or you call the animation module with `--gif`,
a GIF version of the animation is written to:

    rl_8puzzle/solution_3d.gif

This is convenient for previews or embedding in other documents.

---

## ğŸ§ª Tests

To run the tests:

    pytest

The tests cover:

- Environment behavior (valid/invalid moves, reaching the goal, etc.)
- Basic properties of Q-learning and its ability to improve the policy

---

## ğŸ”„ How â€œnew puzzle every runâ€ works

The `runner.py` module implements the â€œfresh puzzleâ€ logic:

1. Scrambles the goal using a configurable number of legal moves
   (`scramble_moves`).
2. Uses the current Q-table to solve the scrambled state via the greedy policy.
3. Rejects the puzzle if:
   - The start state is already listed in `used_start_states.json`, or
   - The solution length is less than `min_moves` (too trivial).
4. Once a suitable puzzle is found:
   - The start state is appended to `used_start_states.json`.
   - A smooth animation is generated and saved as MP4 (and optionally GIF).

To reset the history and allow repeats, simply delete:

    rl_8puzzle/used_start_states.json

---

## ğŸ“„ License

This project is released under the **MIT License**. You are free to use, modify,
and distribute it.

---

## â­ If you like this projectâ€¦

Please consider starring the repository â€” it really helps! â­
